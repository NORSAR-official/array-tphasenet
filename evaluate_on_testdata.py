"""
Evaluation Script for Seismic Phase Detection Models
====================================================

This script evaluates trained seismic phase detection models on test data by computing
comprehensive performance metrics and generating diagnostic visualizations.

Purpose
-------
Loads predictions from trained models and compares them against ground truth labels
to compute precision, recall, F1 scores, and temporal residuals for P-wave and S-wave
phase picks. Supports various evaluation modes including:
- Threshold optimization via precision-recall curves
- SNR-based filtering and analysis
- Distance-based performance evaluation
- Station-specific and catalog-specific metrics
- Theoretical arrival time comparison
- Array station grouping and combination

Workflow
--------
1. Configuration Loading
   - Load training configuration (model architecture, data settings)
   - Load prediction configuration (post-processing settings)
   - Construct model name from configuration parameters

2. Data Loading
   - Load predictions from .npz file generated by train.py
   - Extract ground truth labels, predictions, and metadata
   - Process station names and create event identifiers

3. Model-Specific Processing
   - Handle different model output formats (standard, split-output, beam labels)
   - Split predictions into P and S phase channels

4. Optional Metadata Integration
   - Load arrival metadata (distance, SNR, back-azimuth)
   - Load event metadata (origin time, catalog)
   - Map metadata to prediction events

5. Optional Filtering and Grouping
   - Array station combination (group predictions from array elements)
   - Common event filtering (compare with another model)
   - Station-based filtering
   - SNR-based filtering

6. Threshold Determination
   - Compute optimal thresholds via precision-recall curves
   - Or use fixed thresholds from configuration

7. Performance Metrics Calculation
   - Overall metrics: precision, recall, F1 score
   - Temporal residuals (mean and standard deviation)
   - Combined arrival metrics (P+S)

8. Visualization and Reporting
   - Residual distribution histograms
   - Metadata-based performance plots (distance, station, catalog)
   - Unpicked event analysis (distance-time plots)
   - Theoretical arrival comparison
   - Save results to text files and PNG figures

Configuration Files
-------------------
Training Config (e.g., config.yaml):
    - model.type: Model architecture (transphasenet, splitoutput*, etc.)
    - data.input_dataset_name: Dataset identifier
    - data.test_years: Years to evaluate
    - normalization.channel_mode: Normalization strategy
    - run.outputdir: Output directory for results

Evaluation Config (within training config):
    - evaluation.optimal_threshold: Compute optimal thresholds
    - evaluation.dt: Time window for pick matching (seconds)
    - evaluation.p_threshold, s_threshold: Fixed detection thresholds
    - evaluation.overall_performance: Compute and save metrics
    - evaluation.vs_metadata: Metadata-based analysis mode
    - evaluation.snr_threshold: SNR filtering range
    - evaluation.theoretical_arrivals: Compare with theoretical times
    - evaluation.unpicked: Generate distance-time plots
    - evaluation.save_fig: Save figures to disk
    - evaluation.only_selected_stations: Station filtering (False or list like ['NO','FI'])

Prediction Config
    - prediciton.combine_array_stations: Array combination method
    - Post-processing parameters for predictions
    - ...


Usage
-----
Basic usage with default config.yaml:
    python evaluate_on_testdata.py

Specify configuration file via command-line:
    python evaluate_on_testdata.py --config config_1stat.yaml

The script will:
    1. Load the specified configuration (from command-line or default)
    2. Construct model name from config parameters
    3. Load predictions from outputs/predictions_{model}.npz
    4. Compute and display metrics
    5. Generate plots (interactive and/or saved to disk)
    6. Save text report to output/performance_{model}.txt

Output Files
------------
Text Reports:
    output/performance_{model}.txt
        Overall performance metrics and thresholds

Figures:
    output/residuals_{model}.png
        Histogram of temporal residuals for P and S waves
    
    output/performance_vs_{metadata}_{model}.png
        Performance metrics vs distance/station/catalog
    
    output/missing_picks_{model}.png
        Distance-time plots showing pick coverage

Notes
-----
- Config file can be specified via --config command-line argument
- Some features require metadata CSV files in specific locations
- Array station grouping with beamforming requires array geometry configuration
- Run with --help to see command-line options

Dependencies
------------
- numpy, pandas: Data handling
- matplotlib: Plotting
- omegaconf: Configuration management
- obspy: Seismological time utilities
- h5py: HDF5 file reading (if needed)
- utils: Custom evaluation functions and prediction post-processing
- setup_config: Configuration utilities

See Also
--------
train.py : Model training script that generates predictions

"""

import argparse
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.axes_grid1.inset_locator import inset_axes
from omegaconf import OmegaConf
from setup_config import get_config_dir, dict_to_namespace
from utils import (
    calculate_residual, js_divergence, recall, precision, f1_score,
    plot_L_curve, create_grid, EventWindowEval, group_and_combine_phase_detections
)
from collections import defaultdict
import warnings
import glob
warnings.filterwarnings("ignore")

# Set matplotlib font size for all plots
plt.rcParams.update({'font.size': 13})

def load_metadata(cfg, events):
    """
    Load arrival and event metadata from CSV files and map to event data.
    
    Parameters
    ----------
    cfg : namespace
        Configuration object containing:
        - data.input_dataset_name : Dataset identifier for file matching
        - data.test_years : List of test years to load
        - evaluation.vs_metadata : Whether to load catalog information
        - evaluation.theoretical_arrivals : Whether to load arrival times
    
    events : namespace
        Event data object to be augmented with metadata fields:
        - arrival_id : Array of arrival IDs (input)
        - ids : Array of event IDs (input)
        - distance : Assigned from arrival metadata (output)
        - first_arrival : First arrival time (output, if theoretical_arrivals)
        - last_arrival : Last arrival time (output, if theoretical_arrivals)
        - catalog : Bulletin/catalog name (output, if vs_metadata)
        - orig_time : Event origin time (output, if theoretical_arrivals)
    
    Returns
    -------
    df : pandas.DataFrame
        Arrival metadata dataframe (for SNR filtering if needed)
    arrival_ids_csv : list
        List of arrival IDs from CSV for lookup
    
    Notes
    -----
    - Metadata CSV files are expected in input dir
    - Arrival files: {dataset_name}_{year}_arrivals*.csv
    - Event files: metadata_{year}.csv
    - Uses first arrival ID for events with multiple arrivals
    - For theoretical arrivals, assumes arrivals are sorted by time
    """

    import pandas as pd
    csvpath = f'{cfg.data.inputdir}/'
    df = pd.DataFrame()
    
    # Load arrival metadata CSV files for each test year
    for testyear in cfg.data.test_years:
        datafiles = glob.glob(csvpath+cfg.data.input_dataset_name+'_'+str(testyear)[:4]+'_arrivals*.csv')
        if len(datafiles)>0 : datafile = datafiles[0]
        else : 
            print("No metadata arrival csv file found for setting!")
            exit()
        df = pd.concat([df, pd.read_csv(datafile)],ignore_index=True)
    
    # Create arrival ID lookup for matching
    arrival_ids_csv=[ev+'_'+str(ar) for ev,ar in zip(df['event_id'],df['arrival_id'])]
    
    # Map arrival metadata to events
    # Note: Uses first arrival ID for events with multiple arrivals
    idx = [arrival_ids_csv.index(arids.decode("utf-8").strip().split(',')[0]) for arids in events.arrival_id]
    
    # this is for main event - not for added overlapping arrivals !
    events.distance = df['distance'][idx].values
    
    # Load theoretical arrival times if requested
    if cfg.evaluation.theoretical_arrivals :
        # assumes that arrivals are sorted (not true for added overlapping arrivals)
        events.first_arrival = df['time'][idx].values 
        
        # to get last arrival I have to drop added overlapping arrivals
        # Find the last arrival that belongs to the main event
        tmp = [arids.decode("utf-8").strip().split(',') for arids in events.arrival_id]
        idx = [arrival_ids_csv.index(arids[max(loc for loc, val in enumerate(arids) \
                if '_'.join((arids[0].split('_')[0],arids[0].split('_')[1],arids[0].split('_')[2])) in val)]) \
                for arids in tmp]
        events.last_arrival = df['time'][idx].values
    
    # Load event-level metadata
    evdf = pd.DataFrame()
    for testyear in cfg.data.test_years:
        datafile = 'metadata_'+str(testyear)[:4]+'.csv'
        evdf = pd.concat([evdf, pd.read_csv(glob.glob(csvpath+datafile)[0])],ignore_index=True)
    evdf.rename(columns={'Unnamed: 0': 'event_id'},inplace=True)
    
    # this is for main event - not for added overlapping arrivals !
    if cfg.evaluation.vs_metadata :
        # Map catalog (bulletin) information to events
        events.catalog = evdf['bulletin'][[list(evdf['event_id']).index(i.decode("utf-8")) for i in events.ids]].values
    if cfg.evaluation.theoretical_arrivals :
        # Map origin time to events for theoretical arrival calculation
        events.orig_time = evdf['origin_time'][[list(evdf['event_id']).index(i.decode("utf-8")) for i in events.ids]].values
    
    # Return dataframe and arrival IDs for SNR filtering if needed
    return df, arrival_ids_csv


if __name__ == '__main__':

    # ═══════════════════════════════════════════════════════════════════════════
    # COMMAND-LINE ARGUMENT PARSING
    # ═══════════════════════════════════════════════════════════════════════════
    
    parser = argparse.ArgumentParser(
        description='Evaluate seismic phase detection model on test data.',
        epilog='Example: python evaluate_on_testdata.py --config model_configs/config_array9arces.yaml'
    )
    
    parser.add_argument(
        '--config',
        type=str,
        default='config.yaml',
        help='Path to training configuration file, relative to working directory (default: config.yaml)'
    )
    
    args = parser.parse_args()
    
    only_selected_stations = False
    
    # ═══════════════════════════════════════════════════════════════════════════
    # CONFIGURATION FILE LOADING
    # ═══════════════════════════════════════════════════════════════════════════
    
    print('Reading config ...')
    
    # Configuration file from command-line or default
    configfile = args.config
    
    # Use config file as-is (from working directory or with explicit path)
    # No automatic prepending of model_configs/

    # ═══════════════════════════════════════════════════════════════════════════
    # LOAD CONFIGURATIONS
    # ═══════════════════════════════════════════════════════════════════════════
    
    # Load training configuration (model architecture, data paths, etc.)
    args = OmegaConf.load(configfile)
    args_dict = OmegaConf.to_container(args, resolve=True)
    args = OmegaConf.create(args_dict)
    OmegaConf.set_struct(args, False)
    cfg = dict_to_namespace(args)
    
    # Use prediction configuration from main config file
    # Prediction settings are now in cfg.prediction section
    cfg_pred = cfg.prediction

    
    print('Config read.')

    # ═══════════════════════════════════════════════════════════════════════════
    # CONFIGURATION ADJUSTMENTS
    # ═══════════════════════════════════════════════════════════════════════════
    
    # Override only_selected_stations from config file if specified
    # This allows setting station filtering in the config file under evaluation.only_selected_stations
    if hasattr(cfg, 'evaluation') and hasattr(cfg.evaluation, 'only_selected_stations'):
        only_selected_stations = cfg.evaluation.only_selected_stations
        if only_selected_stations:
            print(f'Station filtering from config: {only_selected_stations}')
    
    # Set default value for array channel extraction if not present
    if not hasattr(cfg.data, "extract_array_channels"): 
        setattr(cfg.data, "extract_array_channels", False)
    
    # ═══════════════════════════════════════════════════════════════════════════
    # MODEL NAME CONSTRUCTION
    # ═══════════════════════════════════════════════════════════════════════════
    # Build model name string used to locate prediction files
    # Format varies based on configuration options (array, beam labels, etc.)
    
    # Base model name
    if cfg.data.extract_array_channels : 
        model = f'array{cfg.data.setname}_{cfg.model.type}'
    else : 
        model = f'{cfg.data.input_dataset_name}_{cfg.model.type}'
    
    # Modify for holdout evaluation
    if cfg.data.holdout :
        model = f'{cfg.data.input_dataset_name}_holdout2_{cfg.model.type}'
    
    # Modify for local normalization
    if cfg.normalization.channel_mode == 'local' :
        model = f'{cfg.data.input_dataset_name}_localnorm_{cfg.model.type}'

    # Handle custom output names
    if not hasattr(cfg.run, "custom_outname"): 
        setattr(cfg.run, "custom_outname", False)
    if cfg.run.custom_outname :
        model = f'{cfg.data.input_dataset_name}_{cfg.run.custom_outname}_{cfg.model.type}'
    if cfg.run.custom_outname and cfg.data.extract_array_channels :
        model = f'array{cfg.data.setname}_{cfg.run.custom_outname}_{cfg.model.type}'
    
    # Add model variant suffix if predicting with different model
    if cfg.run.predict_with_other_model :
        model += f'_{cfg.run.predict_with_other_model}'

    # Initialize suffix for additional output filename modifications
    suffix = ''
    if len(cfg.data.test_years)>1 :
        #suffix += f'_{cfg.data.test_years[0]}-{cfg.data.test_years[1]}'
        model += f'_{cfg.data.test_years[0]}'

    # ═══════════════════════════════════════════════════════════════════════════
    # LOAD PREDICTIONS
    # ═══════════════════════════════════════════════════════════════════════════
    
    print('Loading predictions...')
    
    pred = np.load(f'{cfg.run.outputdir}/predictions/predictions_{model}.npz')
    eval_output_dir='./'

    # Extract data from prediction file
    true = pred['y']         # Ground truth labels
    ids = pred['ids']        # Event IDs
    
    # Process station names
    stations = np.array([stat.decode("utf-8") for stat in pred['stations']])
    arrival_ids = pred['arids']  # Arrival IDs for matching with metadata
    
    # Create event metadata container
    events = EventWindowEval(ids=ids,station=stations,arrival_id=arrival_ids)
    
    # Get model predictions
    pred = pred['yhat']
    
    # Create unique identifiers combining event ID and station code
    ids_uniq = [i.decode("utf-8")+'_'+s for i,s in zip(events.ids,events.station)]
    
    print('Finished')

    print("Events in test data file:",len(events.ids))
    print("Events in batches used for prediction:",len(pred))

    # ═══════════════════════════════════════════════════════════════════════════
    # MODEL-SPECIFIC OUTPUT PROCESSING
    # ═══════════════════════════════════════════════════════════════════════════
    
    # Split predictions by model type
    # Different model architectures have different output channel layouts
    if cfg.model.type.startswith('splitoutput'):
        # Split-output models: [p_noise, p_pick, s_noise, s_pick]
        p_yte, s_yte = np.split(true, 2, axis=-1)
        _, p_pred,_, s_pred = np.split(pred, 4, axis=-1)
    else :
        # Standard models: [noise, p_pick, s_pick]
        d_yte, p_yte, s_yte = np.split(true, 3, axis=-1)
        d_pred, p_pred, s_pred = np.split(pred, 3, axis=-1)
    
    # Get time window for pick matching from config
    dt = cfg.evaluation.dt

    # ═══════════════════════════════════════════════════════════════════════════
    # METADATA LOADING (OPTIONAL)
    # ═══════════════════════════════════════════════════════════════════════════
    # Load arrival and event metadata if needed for:
    # - Distance-based analysis (vs_metadata)
    # - Theoretical arrival times (theoretical_arrivals)
    # - Array station grouping (combine_array_stations) when doing ensemble detection with beamforming
    # - SNR filtering (snr_threshold)
    
    if (cfg.evaluation.vs_metadata or cfg.evaluation.theoretical_arrivals or 
        (cfg_pred.combine_array_stations and cfg_pred.combine_array_stations == 'beam') or cfg.evaluation.snr_threshold):
        df, arrival_ids_csv = load_metadata(cfg, events)
    else:
        df, arrival_ids_csv = None, None

    # ═══════════════════════════════════════════════════════════════════════════
    # ARRAY STATION GROUPING (OPTIONAL)
    # ═══════════════════════════════════════════════════════════════════════════
    # Combine predictions from multiple array elements using array processing
    
    if cfg_pred.combine_array_stations :
        # Map back-azimuth information for array processing
        # this is for main event - not for added overlapping arrivals !
        if cfg_pred.combine_array_stations == 'beam' :
            idx = [arrival_ids_csv.index(arids.decode("utf-8").strip().split(',')[0]) for arids in events.arrival_id]
            events.baz = df['baz'][idx].values
            # Load array geometries from JSON file
            from beamforming import load_array_geometries
            geometry_file = getattr(cfg_pred, 'geometry_file', None)
            try:
                array_geometries = load_array_geometries(geometry_file)
                print(f"[INFO] Loaded array geometries for: {list(array_geometries.keys())}")
            except FileNotFoundError as e:
                print(f"WARNING: Could not load array geometries: {e}")
                print("Falling back to stack method instead.")
                cfg_pred.combine_array_stations = 'stack'
                array_geometries = None
        else :
            array_geometries = None
            events.baz = False
        
        print("Events before array grouping:",len(p_pred))
        # Group and combine detections from array elements
        p_pred,s_pred,idx = group_and_combine_phase_detections(p_pred,s_pred,events.ids,ids_uniq,
                                    events.station,cfg,cfg_pred,array_geometries,events.baz)
        print("Events after array grouping:",len(p_pred))
        
        # Add grouping method to output filename
        suffix += '_'+cfg_pred.combine_array_stations
        
        # Filter events and labels to matched indices
        events.remove_events(idx)
        p_yte = p_yte[idx]
        s_yte = s_yte[idx]
        ids_uniq = [idu for i,idu in enumerate(ids_uniq) if i in idx]

    # ═══════════════════════════════════════════════════════════════════════════
    # COMMON EVENT FILTERING (OPTIONAL)
    # ═══════════════════════════════════════════════════════════════════════════
    # Filter to only events that are common between this model and another model
    # Useful for fair comparison when models were trained/evaluated differently
    
    if cfg.evaluation.common_events_with_model :
        model2 = cfg.evaluation.common_events_with_model
        suffix += '_'+model2
        
        # Load comparison model predictions
        pred2 = np.load(f'{cfg.run.outputdir}/predictions/predictions_{model2}.npz')
        
        # Process comparison model station names
        stations2 = [stat.decode("utf-8") for stat in pred2['stations']]
        ids_uniq2 = [i.decode("utf-8")+'_'+s for i,s in zip(pred2['ids'],stations2)]
        
        # Find intersection of event sets
        both = set(list(ids_uniq)).intersection(list(ids_uniq2))
        idx = [list(ids_uniq).index(x) for x in both if list(ids_uniq).index(x) < len(p_pred)]
        
        print("Common events:",len(idx))
        
        # Filter to common events
        p_yte = p_yte[idx]
        p_pred = p_pred[idx]
        s_yte = s_yte[idx]
        s_pred = s_pred[idx]
        events.remove_events(idx)

    # ═══════════════════════════════════════════════════════════════════════════
    # STATION FILTERING (OPTIONAL)
    # ═══════════════════════════════════════════════════════════════════════════
    # Filter to only specified stations if only_selected_stations is set
    
    if only_selected_stations :
        idx=[i for i,stat in enumerate(stations) if stat in only_selected_stations and i < len(p_yte)]  
        p_yte = p_yte[idx]
        p_pred = p_pred[idx]
        s_yte = s_yte[idx]
        s_pred = s_pred[idx]
        events.remove_events(idx)

    # ═══════════════════════════════════════════════════════════════════════════
    # THRESHOLD DETERMINATION
    # ═══════════════════════════════════════════════════════════════════════════
    # Either compute optimal thresholds from precision-recall curves
    # Or use fixed thresholds from configuration
    
    if cfg.evaluation.optimal_threshold :
        print("Plotting recall-precision curves for optimal thresholds")
        # Compute optimal thresholds that maximize F1 score
        thr_opt_p,thr_opt_s,fig = plot_L_curve(p_yte, p_pred,s_yte, s_pred, model+suffix, cfg,eval_output_dir=eval_output_dir)
        fig.show()
        plt.close()
    else :
        # Use fixed thresholds from configuration
        thr_opt_p=cfg.evaluation.p_threshold
        thr_opt_s=cfg.evaluation.s_threshold
    
    print("Thresholds:",thr_opt_p,thr_opt_s)

    # ═══════════════════════════════════════════════════════════════════════════
    # SNR FILTERING (OPTIONAL)
    # ═══════════════════════════════════════════════════════════════════════════
    # Filter events by signal-to-noise ratio to analyze performance vs SNR
    # Note: This affects recall calculations but may skew precision
    
    if cfg.evaluation.snr_threshold :
        print("Getting SNRs...")
        idx=[]
        snrs = []
        
        # Extract SNR for each event from metadata
        for i,arids in enumerate(events.arrival_id) :
            # Get SNR for all arrivals in this event
            snrs += [df['snr'][arrival_ids_csv.index(arid)] for arid in arids.decode("utf-8").split(',')]
            
            # Aggregate SNR based on specified mode (max or min)
            if cfg.evaluation.snr_mode == 'max' :
                snr = max([df['snr'][arrival_ids_csv.index(arid)] for arid in arids.decode("utf-8").split(',')])
            if cfg.evaluation.snr_mode == 'min' :
                snr = min([df['snr'][arrival_ids_csv.index(arid)] for arid in arids.decode("utf-8").split(',')])
            
            # Keep events within specified SNR range
            if snr > cfg.evaluation.snr_threshold[0] and snr < cfg.evaluation.snr_threshold[1] and i < len(p_pred) :
                idx.append(i)
        print(f'Keeping {len(idx)} arrivals from {len(events.arrival_id)} after SNR filter')

        # Show SNR distribution
        plt.hist(snrs,range=(0,50),bins=50)
        plt.xlabel('SNR')
        plt.ylabel('Counts')
        plt.show()

        # this makes only sense for calculation of Recall to see how SNR affects it
        # this will always decrease precision unless I also remove low SNR detections
        p_yte = p_yte[idx]
        p_pred = p_pred[idx]
        s_yte = s_yte[idx]
        s_pred = s_pred[idx]
        events.remove_events(idx)

    # ═══════════════════════════════════════════════════════════════════════════
    # OVERALL PERFORMANCE METRICS
    # ═══════════════════════════════════════════════════════════════════════════
    # Compute and report precision, recall, F1, and residuals for P and S waves
    
    if cfg.evaluation.overall_performance :
        # Build performance report text
        if cfg.evaluation.snr_threshold :
            text = 'Using SNR limit! This makes only sense for recall evaluation. ' 
            text += 'Precision will always decrease!\n'
        else : 
            text = ''
        
        text+=f'Optimal thresholds used: {thr_opt_p},{thr_opt_s}\n'
        
        # P-wave metrics
        text+=f'P samples {np.count_nonzero(p_yte == 1)}\n'
        text+=f'P Precision ({dt}s) {round(precision(p_yte,p_pred,cfg,dt=dt,th=thr_opt_p, livemode=True), 2)}\n'
        text+=f'P Recall ({dt}s) {round(recall(p_yte, p_pred, cfg, dt=dt, th=thr_opt_p, livemode=True), 2)}\n'
        text+=f'P F1 ({dt}s) {round(f1_score(p_yte, p_pred, cfg, dt=dt, th=thr_opt_p, livemode=True), 2)}\n'
        #text+=f'P JS ({dt}s) {round(js_divergence(p_yte.squeeze(), p_pred.squeeze()), 2)}\n'
        res = calculate_residual(p_yte, p_pred,cfg,dt=dt, th=thr_opt_p,dt_limit=True)
        text+=f'P Residual ({dt}s) {round(np.nanmean(res), 2)}\n'
        text+=f'P Residual Std ({dt}s) {round(np.nanstd(res), 2)}\n'
        text+='\n'
        
        # S-wave metrics
        text+=f'S samples {np.count_nonzero(s_yte == 1)}\n'
        text+=f'S Precision ({dt}s) {round(precision(s_yte,s_pred,cfg,dt=dt,th=thr_opt_s, livemode=True), 2)}\n'
        text+=f'S Recall ({dt}s) {round(recall(s_yte, s_pred,cfg,dt=dt,th=thr_opt_s, livemode=True), 2)}\n'
        text+=f'S F1 ({dt}s) {round(f1_score(s_yte, s_pred,cfg,dt=dt,th=thr_opt_s, livemode=True), 2)}\n'

        # Combined arrival metrics (P+S)
        text+=f'Ar Precision ({dt}s) {round(precision(np.add(p_yte,s_yte),np.add(p_pred,s_pred),cfg,dt=dt,th=thr_opt_s, livemode=True), 2)}\n'
        text+=f'Ar Recall ({dt}s) {round(recall(np.add(p_yte,s_yte),np.add(p_pred,s_pred),cfg,dt=dt,th=thr_opt_s, livemode=True), 2)}\n'
        text+=f'Ar F1 ({dt}s) {round(f1_score(np.add(p_yte,s_yte),np.add(p_pred,s_pred),cfg,dt=dt,th=thr_opt_s, livemode=True), 2)}\n'

        #text+=f'S JS ({dt}s) {round(js_divergence(s_yte.squeeze(), s_pred.squeeze()), 2)}\n'
        res = calculate_residual(s_yte, s_pred,cfg,dt=dt, th=thr_opt_s,dt_limit=True)
        text+='\n'
        text+=f'S Residual ({dt}s) {round(np.nanmean(res), 2)}\n'
        text+=f'S Residual Std ({dt}s) {round(np.nanstd(res), 2)}\n'
        print(text)
        
        # Save report to file
        fout=open(f'{cfg.run.outputdir}/{eval_output_dir}/performance_{model}{suffix}.txt','w')
        fout.writelines(text)
        fout.close()
        
        # Plot residual distributions
        resp = calculate_residual(p_yte, p_pred,cfg,dt=dt, th=thr_opt_p)
        ress = calculate_residual(s_yte, s_pred,cfg,dt=dt, th=thr_opt_s)
        plt.hist(resp,bins=1600,alpha=0.5,label='P waves')
        plt.hist(ress,bins=1600,alpha=0.5,label='S waves')
        plt.yscale('log')
        plt.xlim(-10,10)
        # Reference lines at 0 and ±2 seconds
        plt.plot([0.0,0.0],[0.0,7000],linestyle='dashed',c='black',label=None)
        plt.plot([-2.0,-2.0],[0.0,7000],linestyle='dashed',c='black',label=None)
        plt.plot([2.0,2.0],[0.0,7000],linestyle='dashed',c='black',label=None)
        plt.xlabel('Residual (s)')
        plt.ylabel('Counts')
        plt.legend()
        if cfg.evaluation.save_fig : 
            plt.savefig(f'{cfg.run.outputdir}/{eval_output_dir}/residuals_{model}{suffix}.png')
        plt.show()

    # ═══════════════════════════════════════════════════════════════════════════
    # THEORETICAL ARRIVAL COMPARISON (OPTIONAL)
    # ═══════════════════════════════════════════════════════════════════════════
    # Compare model performance against theoretical P and S arrival times
    # calculated from origin time, distance, and standard velocity models
    
    if cfg.evaluation.theoretical_arrivals :
        from obspy import UTCDateTime
        # note that this does not work for overlaping events
        #p_yte_theo = np.zeros(p_yte.shape)
        #s_yte_theo = np.zeros(s_yte.shape)
        # better: keep all picked arrivlas and add theoretical ones
        p_yte_theo = np.copy(p_yte)
        s_yte_theo = np.copy(s_yte)
        tmp = []
        tmp2 = []
        
        # Calculate theoretical arrival times for each event
        for i,p in enumerate(p_yte):
            # Velocity models (km/s)
            vpg = 6.5  # P-wave crustal velocity (Pg)
            vpn = 7.8  # P-wave mantle velocity (Pn)
            pg_time = UTCDateTime(events.orig_time[i]) + events.distance[i] / vpg
            pn_time = UTCDateTime(events.orig_time[i]) + events.distance[i] / vpn
            vsg = 3.7  # S-wave crustal velocity (Sg)
            vsn = 4.5  # S-wave mantle velocity (Sn)
            sg_time = UTCDateTime(events.orig_time[i]) + events.distance[i] / vsg
            sn_time = UTCDateTime(events.orig_time[i]) + events.distance[i] / vsn
            
            # Calculate window start time accounting for cropping
            window_start = UTCDateTime(events.first_arrival[i]) - 270
            window_start += (UTCDateTime(events.last_arrival[i]) - UTCDateTime(events.first_arrival[i])) / 2
            # adjust for cropping of test data
            window_start += 540//2 - cfg.augment.new_size//2
            #tmp.append(pn_time-UTCDateTime(events.first_arrival[i]))
            #tmp.append(pg_time-UTCDateTime(events.first_arrival[i]))
            #tmp2.append(sn_time-UTCDateTime(events.last_arrival[i]))
            #tmp2.append(sg_time-UTCDateTime(events.last_arrival[i]))

            # Add theoretical picks to label arrays if they fall within the window
            if (int((pg_time-window_start)*cfg.data.sampling_rate) < len(p) and
                int((pg_time-window_start)*cfg.data.sampling_rate) > 0) :
                p_yte_theo[i][int((pg_time-window_start)*cfg.data.sampling_rate)][0] = 1.
            if (int((pn_time-window_start)*cfg.data.sampling_rate) < len(p) and
                int((pn_time-window_start)*cfg.data.sampling_rate) > 0) :
                p_yte_theo[i][int((pn_time-window_start)*cfg.data.sampling_rate)][0] = 1.
            if (int((sg_time-window_start)*cfg.data.sampling_rate) < len(p) and
                int((sg_time-window_start)*cfg.data.sampling_rate) > 0) :
                s_yte_theo[i][int((sg_time-window_start)*cfg.data.sampling_rate)][0] = 1.
            if (int((sn_time-window_start)*cfg.data.sampling_rate) < len(p) and
                int((sn_time-window_start)*cfg.data.sampling_rate) > 0) :
                s_yte_theo[i][int((sn_time-window_start)*cfg.data.sampling_rate)][0] = 1.
        
        # Compute metrics using theoretical picks (with relaxed time windows)
        text=f'Performance with theoretical arrivals:\n'
        dt=7.0  # Larger time window for P waves
        text+=f'P Precision ({dt}s) {round(precision(p_yte_theo,p_pred,cfg,dt=dt,th=thr_opt_p, livemode=True), 2)}\n'
        text+=f'P Recall ({dt}s) {round(recall(p_yte_theo, p_pred, cfg, dt=dt, th=thr_opt_p, livemode=True), 2)}\n'
        dt=5.0  # Smaller time window for S waves
        text+=f'S Precision ({dt}s) {round(precision(s_yte_theo,s_pred,cfg,dt=dt,th=thr_opt_s, livemode=True), 2)}\n'
        text+=f'S Recall ({dt}s) {round(recall(s_yte_theo, s_pred,cfg,dt=dt,th=thr_opt_s, livemode=True), 2)}\n'
        print(text)
        
        # Append to existing performance report
        fout=open(f'{cfg.run.outputdir}/{eval_output_dir}/performance_{model}{suffix}.txt','a')
        fout.writelines(text)
        fout.close()
        #plt.hist(tmp,range=(-50,50),bins=100)
        #plt.hist(tmp2,range=(-50,50),bins=100)
        #plt.show()


    # ═══════════════════════════════════════════════════════════════════════════
    # METADATA-BASED PERFORMANCE ANALYSIS (OPTIONAL)
    # ═══════════════════════════════════════════════════════════════════════════
    # Analyze performance as a function of distance, station, or catalog
    
    # Setup distance bins if distance-based analysis requested
    if cfg.evaluation.vs_metadata == 'distance' :
        dist_bins = {}
        dist_bins['0-9999999'] = {}    # All distances
        dist_bins['0-100'] = {}         # 0-100 km
        dist_bins['100-200'] = {}       # 100-200 km
        dist_bins['200-300'] = {}       # 200-300 km
        dist_bins['300-400'] = {}       # 300-400 km
        dist_bins['400-500'] = {}       # 400-500 km
        dist_bins['500-1000'] = {}      # 500-1000 km
        dist_bins['1000-9999999'] = {}  # >1000 km
        dist_bin_labels = ['All','0-1','1-2','2-3','3-4','4-5','5-10','>10']  # Labels in 100km units
        
    if cfg.evaluation.vs_metadata :
        # Setup figure with dual y-axes (metrics + counts)
        clist = plt.rcParams['axes.prop_cycle'].by_key()['color']
        fig, (ax1,ax2) = plt.subplots(1,2,figsize=(14,5))
        
        # Determine metadata grouping variable
        if cfg.evaluation.vs_metadata == 'station' :
            metadata_list = sorted(list(set(list(events.station))))
            ax1.set_xlabel('Station')
            ax2.set_xlabel('Station')
        if cfg.evaluation.vs_metadata == 'catalog' :
            metadata_list = sorted(list(set(list(events.catalog))))
            ax1.set_xlabel('Catalog')
            ax2.set_xlabel('Catalog')
        if cfg.evaluation.vs_metadata == 'distance' :
            metadata_list = [key for key in dist_bins]
            ax1.set_xlabel('Distance (km*100)')
            ax2.set_xlabel('Distance (km*100)')
        
        # Setup twin axes for pick counts (shown in gray)
        ax3 = ax1.twinx()
        ax4 = ax2.twinx()
        ax3.set_ylim(0,10000)
        ax4.set_ylim(0,10000)
        ax3.set_ylabel('P phase pick counts test data')
        ax4.set_ylabel('S phase pick counts test data')
        ax3.tick_params(axis='y', labelcolor='grey')
        ax4.tick_params(axis='y', labelcolor='grey')
        ax3.yaxis.label.set_color('grey')
        ax4.yaxis.label.set_color('grey')
        
        # Setup primary axes for performance metrics
        ax1.set_ylim(0,1.1)
        ax2.set_ylim(0,1.1)
        ax1.set_xlim(0,len(metadata_list)+1)
        ax2.set_xlim(0,len(metadata_list)+1)
        ax1.set_ylabel('Performance metric value')
        ax2.set_ylabel('Performance metric value')
        
        # Compute metrics for each metadata group
        data = defaultdict(list)
        for item in metadata_list :
            # Select events matching this metadata value
            if cfg.evaluation.vs_metadata == 'station' : 
                idx=np.where(events.station == item)[0]
            if cfg.evaluation.vs_metadata == 'catalog' : 
                idx=np.where(events.catalog == item)[0]
            if cfg.evaluation.vs_metadata == 'distance' : 
                d1 = float(item.split('-')[0])
                d2 = float(item.split('-')[1])
                idx=np.where((events.distance > d1) & (events.distance <= d2))[0]
            idx=idx[np.where(idx<len(p_yte))[0]]
            
            # Compute P-wave metrics for this group
            data['counts_p'].append(np.count_nonzero(p_yte[idx] == 1))
            if len(p_yte[idx]) > 0:
                data['prec_p'].append(round(precision(p_yte[idx],p_pred[idx],cfg, dt=dt, th=thr_opt_p,livemode = True), 2))
                data['rec_p'].append(round(recall(p_yte[idx],p_pred[idx],cfg, dt=dt, th=thr_opt_p,livemode = True), 2))
            
            # Compute S-wave metrics for this group
            data['counts_s'].append(np.count_nonzero(s_yte[idx] == 1))
            if len(s_yte[idx]) > 0:
                data['prec_s'].append(round(precision(s_yte[idx], s_pred[idx], cfg, dt=dt, th=thr_opt_s, livemode = True), 2))
                data['rec_s'].append(round(recall(s_yte[idx], s_pred[idx], cfg, dt=dt, th=thr_opt_s, livemode = True), 2))
        
        # Plot bars for counts and metrics
        xpos = np.array([i+1 for i,_ in enumerate(metadata_list)])
        ax3.bar(xpos,data['counts_p'],color='grey')
        ax1.bar(xpos,data['prec_p'],width=0.1,color=clist[0],label='P Prec')
        ax1.bar(xpos+0.1,data['rec_p'],width=0.1,color=clist[1],label='P Rec')
        ax4.bar(xpos,data['counts_s'],color='grey')
        ax2.bar(xpos,data['prec_s'],width=0.1,color=clist[0],label='S Prec')
        ax2.bar(xpos+0.1,data['rec_s'],width=0.1,color=clist[1],label='S Rec')
        
        # Set x-axis labels
        ax1.set_xticks(xpos) 
        ax2.set_xticks(xpos)             
        if cfg.evaluation.vs_metadata == 'distance' :
            ax1.set_xticklabels(dist_bin_labels)
            ax2.set_xticklabels(dist_bin_labels)
        else :
            ax1.set_xticklabels(metadata_list)
            ax2.set_xticklabels(metadata_list)
        
        ax1.legend(loc='lower left')
        ax2.legend(loc='lower left')
        
        # Ensure performance bars appear in front of count bars
        ax1.set_zorder(ax3.get_zorder()+1)
        ax1.patch.set_visible(False)
        ax2.set_zorder(ax4.get_zorder()+1) 
        ax2.patch.set_visible(False)
        
        fig.tight_layout()
        
        # Save figure
        if cfg.evaluation.save_fig :
            figoutname = f'performance_vs_{cfg.evaluation.vs_metadata}_{model}{suffix}'
            plt.savefig(f'{cfg.run.outputdir}/{eval_output_dir}/{figoutname}.png')
        plt.show()

    # ═══════════════════════════════════════════════════════════════════════════
    # UNPICKED EVENTS ANALYSIS (OPTIONAL)
    # ═══════════════════════════════════════════════════════════════════════════
    # Generate distance-time plots showing where model makes picks vs ground truth
    # Helps identify systematic biases or gaps in pick coverage
    
    if cfg.evaluation.unpicked :
        # Create prediction dictionary for grid analysis
        pred_dict={}
        # different stations can have same event ID!
        for evid,p,s,tp,ts,stat in zip(events.ids,p_pred,s_pred,p_yte,s_yte,events.station):
            pred_dict[evid.decode("utf-8")+'_'+stat] = {}
            pred_dict[evid.decode("utf-8")+'_'+stat]['yhat_p']=p
            pred_dict[evid.decode("utf-8")+'_'+stat]['yhat_s']=s
            pred_dict[evid.decode("utf-8")+'_'+stat]['y_p']=tp
            pred_dict[evid.decode("utf-8")+'_'+stat]['y_s']=ts
        
        # Setup distance bins for grid
        ddist = 10  # 10 km bins
        dist_bins = [[d,d+ddist] for d in np.arange(0,5000,ddist)]
        nsamples = int(5*60*cfg.data.sampling_rate)  # 5 minute window
        vmax = 0.2  # Color scale maximum
        
        # Create 3x2 subplot figure
        fig, ((ax1,ax2),(ax3,ax4),(ax5,ax6)) = plt.subplots(3,2,figsize=(8,9))
        cmap = plt.get_cmap('hot_r')
        cmap.set_over('black')
        extent = (0,nsamples/cfg.data.sampling_rate,5000+ddist,0)
        
        # P-wave analysis with reduced velocity
        redvel = 7.01  # km/s (close to Pn velocity)
        
        # Plot labeled P picks (single pick per event)
        grid,grid_s = create_grid(pred_dict,dist_bins,nsamples,events.ids,events.distance,events.station,
                                  'P',cfg,pick=True,redvel=redvel)
        im=ax1.imshow(grid,aspect='auto',origin='upper',extent=extent,vmax=vmax,cmap=cmap,
                      interpolation='none')
        axins1 = inset_axes(
            ax1,
            width="5%",
            height="80%",
            loc="lower left",
        )
        fig.colorbar(im,cax=axins1)
        ax1.imshow(grid,aspect='auto',origin='upper',extent=extent,vmax=vmax,cmap=cmap,interpolation='none')
        
        # Plot predicted P picks
        grid,grid_s = create_grid(pred_dict,dist_bins,nsamples,events.ids,events.distance,events.station,
                                  'P',cfg,pick=False,redvel=redvel)
        ax3.imshow(grid,aspect='auto',origin='upper',extent=extent,vmax=vmax,cmap=cmap,interpolation='none')
        ax5.imshow(grid_s,aspect='auto',origin='upper',extent=extent,vmax=vmax,
                   cmap=cmap,interpolation='none')
        
        # S-wave analysis with reduced velocity
        redvel = 3.51  # km/s (close to Sn velocity)
        
        # Plot labeled S picks
        grid,grid_s = create_grid(pred_dict,dist_bins,nsamples,events.ids,events.distance,events.station,
                                  'S',cfg,pick=True,redvel=redvel)
        ax2.imshow(grid_s,aspect='auto',origin='upper',extent=extent,vmax=vmax,cmap=cmap,
                   interpolation='none')
        
        # Plot predicted S picks
        grid,grid_s = create_grid(pred_dict,dist_bins,nsamples,events.ids,events.distance,events.station,
                                  'S',cfg,pick=False,smask=True,redvel=redvel)
        ax4.imshow(grid,aspect='auto',origin='upper',extent=extent,vmax=vmax,cmap=cmap,interpolation='none')
        ax6.imshow(grid_s,aspect='auto',origin='upper',extent=extent,vmax=vmax,cmap=cmap,
                   interpolation='none')
        
        # Add titles and labels
        ax1.set_title('a) Labeled P picks (single pick)')
        ax2.set_title('d) Labeled S picks (single pick)')
        ax3.set_title('b) Predicted P picks')
        ax4.set_title('e) Predicted P picks')
        ax5.set_title('c) Predicted S picks')
        ax6.set_title('f) Predicted S picks')
        ax5.set_xlabel('Reduced Travel Time (s)')
        ax6.set_xlabel('Reduced Travel Time (s)')
        ax1.set_ylabel('Distance (km)')
        ax3.set_ylabel('Distance (km)')
        ax5.set_ylabel('Distance (km)')
        
        fig.tight_layout()
        
        # Save figure
        if cfg.evaluation.save_fig :
            plt.savefig(f'{cfg.run.outputdir}/{eval_output_dir}/missing_picks_{model}{suffix}.png')
        plt.show()
